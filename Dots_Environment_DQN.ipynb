{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dots_Env",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koldo2003/Capstone-Project/blob/main/Dots_Environment_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u9QVVsShC9X"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOoWVxY3nogC"
      },
      "source": [
        "This is a turtorial from Open AI, that we modified to fit the needs of the Dots and Boxes environment and research"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNrNXKI7bINP"
      },
      "source": [
        "If you haven't installed the following dependencies, run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEHR2Ui-lo8O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "937d2937-279c-43dc-b883-5f2a8cc90564"
      },
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
        "# !pip install 'imageio==2.4.0'\n",
        "# !pip install pyvirtualdisplay\n",
        "!pip install tf-agents[reverb]\n",
        "# !pip install pyglet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Waiting f\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rHit:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rHit:4 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Connecting to ppa.launchpa\r                                                                               \rHit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "freeglut3-dev is already the newest version (2.8.1-3).\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.9).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 64 not upgraded.\n",
            "Requirement already satisfied: tf-agents[reverb] in /usr/local/lib/python3.7/dist-packages (0.11.0)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.5.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (7.1.2)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-probability>=0.14.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.14.1)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (3.10.0.2)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.13.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.19.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.15.0)\n",
            "Requirement already satisfied: gym>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.17.3)\n",
            "Requirement already satisfied: dm-reverb~=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.6.1)\n",
            "Requirement already satisfied: tensorflow~=2.7.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (2.7.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from dm-reverb~=0.6.0->tf-agents[reverb]) (0.1.6)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.7/dist-packages (from dm-reverb~=0.6.0->tf-agents[reverb]) (1.3.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents[reverb]) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents[reverb]) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17.0->tf-agents[reverb]) (0.16.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->tf-agents[reverb]) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->tf-agents[reverb]) (0.22.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->tf-agents[reverb]) (1.41.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->tf-agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->tf-agents[reverb]) (3.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->tf-agents[reverb]) (1.6.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->tf-agents[reverb]) (1.1.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->tf-agents[reverb]) (2.7.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->tf-agents[reverb]) (2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->tf-agents[reverb]) (0.37.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->tf-agents[reverb]) (2.7.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->tf-agents[reverb]) (0.4.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->tf-agents[reverb]) (12.0.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->tf-agents[reverb]) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->tf-agents[reverb]) (2.7.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow~=2.7.0->tf-agents[reverb]) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (1.8.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (4.8.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (3.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.14.1->tf-agents[reverb]) (4.4.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMitx5qSgJk1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "outputId": "d972873c-8846-4435-8862-187c4dca9cc4"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "# import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# import PIL.Image\n",
        "# import pyvirtualdisplay\n",
        "import reverb\n",
        "\n",
        "import tensorflow as tf\n",
        "import Dots\n",
        "import random\n",
        "import os\n",
        "import tempfile\n",
        "import shutil \n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import py_driver\n",
        "# from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.utils import common\n",
        "\n",
        "# must import these:\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "\n",
        "tempdir = os.getenv(\"TEST_TMPDIR\", tempfile.gettempdir())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NspmzG4nP3b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7e8e7c96-78c1-4b67-e935-ce5542c7198f"
      },
      "source": [
        "tf.version.VERSION"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.7.0'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmC0NDhdLIKY"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC1kNrOsLSIZ",
        "cellView": "form"
      },
      "source": [
        "num_iterations = 10000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps =   1000# @param {type:\"integer\"}\n",
        "collect_steps_per_iteration =   1# @param {type:\"integer\"}\n",
        "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
        "\n",
        "batch_size =   500# @param {type:\"integer\"}\n",
        "learning_rate = 1e-3  # @param {type:\"number\"}\n",
        "log_interval =   250# @param {type:\"integer\"}\n",
        "\n",
        "num_eval_episodes = 100  # @param {type:\"integer\"}\n",
        "eval_interval =   500# @param {type:\"integer\"}\n",
        "\n",
        "n, m = 1, 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMsJC3DEgI0x"
      },
      "source": [
        "## Environment\n",
        "\n",
        "In Reinforcement Learning (RL), an environment represents the task or problem to be solved. Standard environments can be created in TF-Agents using `tf_agents.environments` suites. TF-Agents has suites for loading environments from sources such as the OpenAI Gym, Atari, and DM Control.\n",
        "\n",
        "We define a custom environment here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYEz-S9gEv2-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca665fba-26b4-484b-da90-ffb68a93d458"
      },
      "source": [
        "def wi(board): # wrap integer\n",
        "  return np.array([int(x) for x in board],dtype=np.int32)\n",
        "\n",
        "class Dots_Env(py_environment.PyEnvironment):\n",
        "\n",
        "  def __init__(self):\n",
        "    self._action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum= m + ((2*m)+1)*n - 1, name='action')\n",
        "    self._observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(m + ((2*m)+1)*n,), dtype=np.int32, minimum=0, maximum=1, name='observation')\n",
        "    self._discount_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.float32, minimum=-1.0, maximum=1.0, name='discount')\n",
        "    self.reset()\n",
        "\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  def discount_spec(self):\n",
        "    return self._discount_spec\n",
        "\n",
        "  def _reset(self):\n",
        "    self._state = Dots.Dots(n,m)\n",
        "    self.last_reward = 0\n",
        "    self._episode_ended = False\n",
        "    return ts.restart(wi(self._state.board))\n",
        "\n",
        "  def _step(self, action):\n",
        "    # If last action ended the game, restart\n",
        "    if self._episode_ended:\n",
        "    #   print('episode ended, resetting')\n",
        "      return self.reset()\n",
        "    \n",
        "    new_move = action % len(self._state.legal_moves())\n",
        "    action = self._state.legal_moves()[new_move]\n",
        "\n",
        "\n",
        "    # If action illegal, just issue a big penalty and stop game\n",
        "    if action not in self._state.legal_moves():\n",
        "    #   print(self._num_moves_done,':',self._state,action,'illegal')\n",
        "      self._episode_ended = True\n",
        "      return ts.termination(wi(self._state.board),-(n*m+1))\n",
        "    # Otherwise make their move and give them the appropriate reward\n",
        "    else:\n",
        "      score = self._state.score()\n",
        "      self._state.play(action)\n",
        "      new_score = self._state.score()\n",
        "      reward = new_score-score\n",
        "      # The move may or may not end the game...\n",
        "      if len(self._state.legal_moves()) > 0:\n",
        "        # print(self._num_moves_done,':',self._state,action,reward)\n",
        "        turn = 1 - reward\n",
        "\n",
        "        while turn!=0:\n",
        "          if len(self._state.legal_moves())==0:\n",
        "            self._episode_ended = True\n",
        "            return ts.termination(wi(self._state.board),reward)\n",
        "          \n",
        "          self._state.play(random.choice(self._state.legal_moves()))\n",
        "          turn = self._state.score() - new_score\n",
        "          new_score = self._state.score()\n",
        "\n",
        "        return ts.transition(wi(self._state.board),reward,discount = 1.0)\n",
        "      else:\n",
        "        # print(self._num_moves_done,':',self._state,action,reward,'done')\n",
        "        self._episode_ended = True\n",
        "        return ts.termination(wi(self._state.board),reward)\n",
        "\n",
        "env = Dots_Env()\n",
        "env.time_step_spec()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TimeStep(\n",
              "{'discount': BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0),\n",
              " 'observation': BoundedArraySpec(shape=(7,), dtype=dtype('int32'), name='observation', minimum=0, maximum=1),\n",
              " 'reward': ArraySpec(shape=(), dtype=dtype('float32'), name='reward'),\n",
              " 'step_type': ArraySpec(shape=(), dtype=dtype('int32'), name='step_type')})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9_lskPOey18"
      },
      "source": [
        "The `environment.step` method takes an `action` in the environment and returns a `TimeStep` tuple containing the next observation of the environment and the reward for the action.\n",
        "\n",
        "The `time_step_spec()` method returns the specification for the `TimeStep` tuple. Its `observation` attribute shows the shape of observations, the data types, and the ranges of allowed values. The `reward` attribute shows the same details for the reward.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exDv57iHfwQV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a780c4f1-9857-4d72-c59a-35039989b522"
      },
      "source": [
        "print('Observation Spec:')\n",
        "print(env.time_step_spec().observation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation Spec:\n",
            "BoundedArraySpec(shape=(7,), dtype=dtype('int32'), name='observation', minimum=0, maximum=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxiSyCbBUQPi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "679b9beb-08d6-4ed4-df9b-11225c28043c"
      },
      "source": [
        "print('Reward Spec:')\n",
        "print(env.time_step_spec().reward)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reward Spec:\n",
            "ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_lHcIcqUaqB"
      },
      "source": [
        "The `action_spec()` method returns the shape, data types, and allowed values of valid actions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bttJ4uxZUQBr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd81efa3-0d94-45e4-dd3a-57bf6d57c2ce"
      },
      "source": [
        "print('Action Spec:')\n",
        "print(env.action_spec())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Spec:\n",
            "BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action', minimum=0, maximum=6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JSc9GviWUBK"
      },
      "source": [
        "Usually two environments are instantiated: one for training and one for evaluation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7brXNIGWXjC"
      },
      "source": [
        "train_py_env = Dots_Env()\n",
        "eval_py_env = Dots_Env()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuUqXAVmecTU"
      },
      "source": [
        "Our environment, like most environments, is written in pure Python. This is converted to TensorFlow using the `TFPyEnvironment` wrapper.\n",
        "\n",
        "The original environment's API uses Numpy arrays. The `TFPyEnvironment` converts these to `Tensors` to make it compatible with Tensorflow agents and policies.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp-Y4mD6eDhF"
      },
      "source": [
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9lW_OZYFR8A"
      },
      "source": [
        "## Agent\n",
        "\n",
        "The algorithm used to solve an RL problem is represented by an `Agent`. TF-Agents provides standard implementations of a variety of `Agents`, including:\n",
        "\n",
        "-   [DQN](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) (used in this tutorial)\n",
        "-   [REINFORCE](https://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)\n",
        "-   [DDPG](https://arxiv.org/pdf/1509.02971.pdf)\n",
        "-   [TD3](https://arxiv.org/pdf/1802.09477.pdf)\n",
        "-   [PPO](https://arxiv.org/abs/1707.06347)\n",
        "-   [SAC](https://arxiv.org/abs/1801.01290)\n",
        "\n",
        "The DQN agent can be used in any environment which has a discrete action space.\n",
        "\n",
        "At the heart of a DQN Agent is a `QNetwork`, a neural network model that can learn to predict `QValues` (expected returns) for all actions, given an observation from the environment.\n",
        "\n",
        "We will use `tf_agents.networks.` to create a `QNetwork`. The network will consist of a sequence of `tf.keras.layers.Dense` layers, where the final layer will have 1 output for each possible action."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgkdEPg_muzV"
      },
      "source": [
        "fc_layer_params = (20,20) # (100, 50)\n",
        "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
        "\n",
        "# Define a helper function to create Dense layers configured with the right\n",
        "# activation and kernel initializer.\n",
        "def dense_layer(num_units):\n",
        "  return tf.keras.layers.Dense(\n",
        "      num_units,\n",
        "      activation=tf.keras.activations.relu,\n",
        "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
        "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
        "\n",
        "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
        "# with `num_actions` units to generate one q_value per available action as\n",
        "# its output.\n",
        "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
        "q_values_layer = tf.keras.layers.Dense(\n",
        "    num_actions,\n",
        "    activation=None,\n",
        "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
        "        minval=-0.03, maxval=0.03),\n",
        "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
        "q_net = sequential.Sequential(dense_layers + [q_values_layer])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z62u55hSmviJ"
      },
      "source": [
        "Now use `tf_agents.agents.dqn.dqn_agent` to instantiate a `DqnAgent`. In addition to the `time_step_spec`, `action_spec` and the QNetwork, the agent constructor also requires an optimizer (in this case, `AdamOptimizer`), a loss function, and an integer step counter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbY4yrjTEyc9"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0KLrEPwkn5x"
      },
      "source": [
        "## Policies\n",
        "\n",
        "A policy defines the way an agent acts in an environment. Typically, the goal of reinforcement learning is to train the underlying model until the policy produces the desired outcome.\n",
        "\n",
        "Agents contain two policies: \n",
        "\n",
        "-   `agent.policy` — The main policy that is used for evaluation and deployment.\n",
        "-   `agent.collect_policy` — A second policy that is used for data collection.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwY7StuMkuV4"
      },
      "source": [
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Qs1Fl3dV0ae"
      },
      "source": [
        "Policies can be created independently of agents. For example, use `tf_agents.policies.random_tf_policy` to create a policy which will randomly select an action for each `time_step`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HE37-UCIrE69"
      },
      "source": [
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
        "                                                train_env.action_spec())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOlnlRRsUbxP"
      },
      "source": [
        "To get an action from a policy, call the `policy.action(time_step)` method. The `time_step` contains the observation from the environment. This method returns a `PolicyStep`, which is a named tuple with three components:\n",
        "\n",
        "-   `action` — the action to be taken (in this case, `0` or `1`)\n",
        "-   `state` — used for stateful (that is, RNN-based) policies\n",
        "-   `info` — auxiliary data, such as log probabilities of actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gCcpXswVAxk"
      },
      "source": [
        "example_environment = tf_py_environment.TFPyEnvironment(Dots_Env())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4DHZtq3Ndis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad86b851-6d12-4c09-ad11-471b797d4c3a"
      },
      "source": [
        "time_step = example_environment.reset()\n",
        "time_step"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TimeStep(\n",
              "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
              " 'observation': <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0]], dtype=int32)>,\n",
              " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
              " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRFqAUzpNaAW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bed03ba8-1692-4d10-d432-57b6b1e55b43"
      },
      "source": [
        "random_policy.action(time_step)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([3], dtype=int32)>, state=(), info=())"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94rCXQtbUbXv"
      },
      "source": [
        "## Metrics and Evaluation\n",
        "\n",
        "The most common metric used to evaluate a policy is the average return. The return is the sum of rewards obtained while running a policy in an environment for an episode. Several episodes are run, creating an average return.\n",
        "\n",
        "The following function computes the average return of a policy, given the policy, environment, and a number of episodes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bitzHo5_UbXy"
      },
      "source": [
        "def compute_avg_return(environment, policy, num_episodes=10,printout=False):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    if printout:\n",
        "      print(\"NEW EPISODE:\\n------------\")\n",
        "      print(time_step)\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      if printout:\n",
        "        print(\"This is the move I played:\",action_step.action.numpy()[0])\n",
        "        print(''.join([str(x) for x in time_step.observation.numpy()]))\n",
        "      episode_return += time_step.reward\n",
        "    if printout:\n",
        "      print(\"REWARD FOR THAT EPISODE:\",episode_return.numpy()[0])\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_snCVvq5Z8lJ"
      },
      "source": [
        "Running this computation on the `random_policy` shows a baseline performance in the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bgU6Q6BZ8Bp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "697c5a55-541d-45d0-d408-aafe1fd98e1f"
      },
      "source": [
        "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.51"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLva6g2jdWgr"
      },
      "source": [
        "## Replay Buffer\n",
        "\n",
        "In order to keep track of the data collected from the environment, we will use [Reverb](https://deepmind.com/research/open-source/Reverb), an efficient, extensible, and easy-to-use replay system by Deepmind. It stores experience data when we collect trajectories and is consumed during training.\n",
        "\n",
        "This replay buffer is constructed using specs describing the tensors that are to be stored, which can be obtained from the agent using agent.collect_data_spec.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX2zGUWJGWAl"
      },
      "source": [
        "table_name = 'uniform_table'\n",
        "replay_buffer_signature = tensor_spec.from_spec(\n",
        "      agent.collect_data_spec)\n",
        "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
        "    replay_buffer_signature)\n",
        "\n",
        "table = reverb.Table(\n",
        "    table_name,\n",
        "    max_size=replay_buffer_max_length,\n",
        "    sampler=reverb.selectors.Uniform(),\n",
        "    remover=reverb.selectors.Fifo(),\n",
        "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
        "    signature=replay_buffer_signature)\n",
        "\n",
        "reverb_server = reverb.Server([table])\n",
        "\n",
        "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "    agent.collect_data_spec,\n",
        "    table_name=table_name,\n",
        "    sequence_length=2,\n",
        "    local_server=reverb_server)\n",
        "\n",
        "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
        "  replay_buffer.py_client,\n",
        "  table_name,\n",
        "  sequence_length=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGNTDJpZs4NN"
      },
      "source": [
        "For most agents, `collect_data_spec` is a named tuple called `Trajectory`, containing the specs for observations, actions, rewards, and other items."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IZ-3HcqgE1z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a69bbc9-91e3-4365-f263-bc97b9561a94"
      },
      "source": [
        "agent.collect_data_spec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Trajectory(\n",
              "{'action': BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(6, dtype=int32)),\n",
              " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
              " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
              " 'observation': BoundedTensorSpec(shape=(7,), dtype=tf.int32, name='observation', minimum=array(0, dtype=int32), maximum=array(1, dtype=int32)),\n",
              " 'policy_info': (),\n",
              " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
              " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy6g1tGcfRlw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "052bf39b-bf9c-426f-e016-4adcf5322bba"
      },
      "source": [
        "agent.collect_data_spec._fields"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('step_type',\n",
              " 'observation',\n",
              " 'action',\n",
              " 'policy_info',\n",
              " 'next_step_type',\n",
              " 'reward',\n",
              " 'discount')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVD5nQ9ZGo8_"
      },
      "source": [
        "## Data Collection\n",
        "\n",
        "Now execute the random policy in the environment for a few steps, recording the data in the replay buffer.\n",
        "\n",
        "Here we are using 'PyDriver' to run the experience collecting loop. You can learn more about TF Agents driver in our [drivers tutorial](https://www.tensorflow.org/agents/tutorials/4_drivers_tutorial)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr1KSAEGG4h9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bd371c7-076e-4616-bf02-178bfd0ceab2"
      },
      "source": [
        "py_driver.PyDriver(\n",
        "    env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      random_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=initial_collect_steps).run(train_py_env.reset())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TimeStep(\n",
              "{'discount': array(0., dtype=float32),\n",
              " 'observation': array([1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
              " 'reward': array(2., dtype=float32),\n",
              " 'step_type': array(2, dtype=int32)}),\n",
              " ())"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84z5pQJdoKxo"
      },
      "source": [
        "The replay buffer is now a collection of Trajectories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wZnLu2ViO4E"
      },
      "source": [
        "# For the curious:\n",
        "# Uncomment to peel one of these off and inspect it.\n",
        "# iter(replay_buffer.as_dataset()).next()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TujU-PMUsKjS"
      },
      "source": [
        "The agent needs access to the replay buffer. This is provided by creating an iterable `tf.data.Dataset` pipeline which will feed data to the agent.\n",
        "\n",
        "Each row of the replay buffer only stores a single observation step. But since the DQN Agent needs both the current and next observation to compute the loss, the dataset pipeline will sample two adjacent rows for each item in the batch (`num_steps=2`).\n",
        "\n",
        "This dataset is also optimized by running parallel calls and prefetching data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba7bilizt_qW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "026d6461-6eae-4f11-8066-9da0c99f8fb3"
      },
      "source": [
        "# Dataset generates trajectories with shape [Bx2x...]\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3,\n",
        "    sample_batch_size=batch_size,\n",
        "    num_steps=2).prefetch(3)\n",
        "\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: (Trajectory(\n",
              "{action: (500, 2),\n",
              " discount: (500, 2),\n",
              " next_step_type: (500, 2),\n",
              " observation: (500, 2, 7),\n",
              " policy_info: (),\n",
              " reward: (500, 2),\n",
              " step_type: (500, 2)}), SampleInfo(key=(500, 2), probability=(500, 2), table_size=(500, 2), priority=(500, 2))), types: (Trajectory(\n",
              "{action: tf.int32,\n",
              " discount: tf.float32,\n",
              " next_step_type: tf.int32,\n",
              " observation: tf.int32,\n",
              " policy_info: (),\n",
              " reward: tf.float32,\n",
              " step_type: tf.int32}), SampleInfo(key=tf.uint64, probability=tf.float64, table_size=tf.int64, priority=tf.float64))>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K13AST-2ppOq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9040e83d-260c-40de-ae03-43cc96a18d08"
      },
      "source": [
        "iterator = iter(dataset)\n",
        "print(iterator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7fda4e16a790>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th5w5Sff0b16"
      },
      "source": [
        "# For the curious:\n",
        "# Uncomment to see what the dataset iterator is feeding to the agent.\n",
        "# Compare this representation of replay data \n",
        "# to the collection of individual trajectories shown earlier.\n",
        "\n",
        "# iterator.next()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBc9lj9VWWtZ"
      },
      "source": [
        "## Training the agent\n",
        "\n",
        "Two things must happen during the training loop:\n",
        "\n",
        "-   collect data from the environment\n",
        "-   use that data to train the agent's neural network(s)\n",
        "\n",
        "This example also periodicially evaluates the policy and prints the current score.\n",
        "\n",
        "The following will take ~5 minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzirA58QIdHs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d03c0e2-1a47-4fff-b8de-43223266a395"
      },
      "source": [
        "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.42"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuD0eT4uIkq9"
      },
      "source": [
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bmh3dDbLIvTT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6534f8d2-6f7c-4be8-f4fa-ae7da45d5769"
      },
      "source": [
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step.\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "returns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pTbJ3PeyF-u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89dde504-b202-4fe1-aa2a-e9c43807e33b"
      },
      "source": [
        "try:\n",
        "  %%time\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# Reset the environment.\n",
        "time_step = train_py_env.reset()\n",
        "\n",
        "# Create a driver to collect experience.\n",
        "collect_driver = py_driver.PyDriver(\n",
        "    env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      agent.collect_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=collect_steps_per_iteration)\n",
        "\n",
        "#sets up checkpointer\n",
        "shutil.rmtree(tempdir)\n",
        "checkpoint_dir = os.path.join(tempdir, 'checkpoint')\n",
        "train_checkpointer = common.Checkpointer(\n",
        "    ckpt_dir=checkpoint_dir,\n",
        "    max_to_keep=1,\n",
        "    agent=agent,\n",
        "    policy=agent.policy,\n",
        "    replay_buffer=replay_buffer\n",
        ")\n",
        "\n",
        "\n",
        "last_avg_return = None # haven't trained yet\n",
        "for _ in range(num_iterations):\n",
        "\n",
        "  # Collect a few steps and save to the replay buffer.\n",
        "  time_step, _ = collect_driver.run(time_step)\n",
        "\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, unused_info = next(iterator)\n",
        "  train_loss = agent.train(experience).loss\n",
        "  \n",
        "  step = agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "\n",
        "    if last_avg_return is None or avg_return >= last_avg_return:\n",
        "      last_avg_return = avg_return\n",
        "      train_checkpointer.save(tf.compat.v1.train.get_global_step()) #save agent\n",
        "      print(f'This is a new best: {avg_return}')\n",
        "    else:\n",
        "      print('Returning to previous policy...')\n",
        "    \n",
        "    train_checkpointer.initialize_or_restore() #restore agent\n",
        "    \n",
        "    returns.append(last_avg_return)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 0 ns, sys: 4 µs, total: 4 µs\n",
            "Wall time: 7.39 µs\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "Instead of:\n",
            "results = tf.foldr(fn, elems, back_prop=False)\n",
            "Use:\n",
            "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "Instead of:\n",
            "results = tf.foldr(fn, elems, back_prop=False)\n",
            "Use:\n",
            "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 250: loss = 0.21376970410346985\n",
            "step = 500: loss = 0.22822412848472595\n",
            "step = 500: Average Return = 0.699999988079071\n",
            "This is a new best: 0.699999988079071\n",
            "step = 750: loss = 0.17425794899463654\n",
            "step = 1000: loss = 0.18558482825756073\n",
            "step = 1000: Average Return = 0.9700000286102295\n",
            "This is a new best: 0.9700000286102295\n",
            "step = 1250: loss = 0.1609320044517517\n",
            "step = 1500: loss = 0.23618772625923157\n",
            "step = 1500: Average Return = 1.0099999904632568\n",
            "This is a new best: 1.0099999904632568\n",
            "step = 1750: loss = 0.13918426632881165\n",
            "step = 2000: loss = 0.13328835368156433\n",
            "step = 2000: Average Return = 0.7699999809265137\n",
            "Returning to previous policy...\n",
            "step = 2250: loss = 0.18023033440113068\n",
            "step = 2500: loss = 0.15701410174369812\n",
            "step = 2500: Average Return = 0.8999999761581421\n",
            "Returning to previous policy...\n",
            "step = 2750: loss = 0.2123592644929886\n",
            "step = 3000: loss = 0.13178138434886932\n",
            "step = 3000: Average Return = 1.0099999904632568\n",
            "This is a new best: 1.0099999904632568\n",
            "step = 3250: loss = 0.13294187188148499\n",
            "step = 3500: loss = 0.13728000223636627\n",
            "step = 3500: Average Return = 0.9100000262260437\n",
            "Returning to previous policy...\n",
            "step = 3750: loss = 0.19904960691928864\n",
            "step = 4000: loss = 0.19437243044376373\n",
            "step = 4000: Average Return = 1.059999942779541\n",
            "This is a new best: 1.059999942779541\n",
            "step = 4250: loss = 0.1755342185497284\n",
            "step = 4500: loss = 0.12480299174785614\n",
            "step = 4500: Average Return = 0.949999988079071\n",
            "Returning to previous policy...\n",
            "step = 4750: loss = 0.12543271481990814\n",
            "step = 5000: loss = 0.12591859698295593\n",
            "step = 5000: Average Return = 1.0700000524520874\n",
            "This is a new best: 1.0700000524520874\n",
            "step = 5250: loss = 0.16021576523780823\n",
            "step = 5500: loss = 0.1800817847251892\n",
            "step = 5500: Average Return = 1.0399999618530273\n",
            "Returning to previous policy...\n",
            "step = 5750: loss = 0.1523672640323639\n",
            "step = 6000: loss = 0.17705337703227997\n",
            "step = 6000: Average Return = 0.949999988079071\n",
            "Returning to previous policy...\n",
            "step = 6250: loss = 0.18317867815494537\n",
            "step = 6500: loss = 0.1406373381614685\n",
            "step = 6500: Average Return = 1.100000023841858\n",
            "This is a new best: 1.100000023841858\n",
            "step = 6750: loss = 0.17438659071922302\n",
            "step = 7000: loss = 0.17961126565933228\n",
            "step = 7000: Average Return = 0.9200000166893005\n",
            "Returning to previous policy...\n",
            "step = 7250: loss = 0.18355774879455566\n",
            "step = 7500: loss = 0.15702864527702332\n",
            "step = 7500: Average Return = 0.8700000047683716\n",
            "Returning to previous policy...\n",
            "step = 7750: loss = 0.12002909183502197\n",
            "step = 8000: loss = 0.1563430279493332\n",
            "step = 8000: Average Return = 1.0\n",
            "Returning to previous policy...\n",
            "step = 8250: loss = 0.12365711480379105\n",
            "step = 8500: loss = 0.14550496637821198\n",
            "step = 8500: Average Return = 0.8600000143051147\n",
            "Returning to previous policy...\n",
            "step = 8750: loss = 0.1554415076971054\n",
            "step = 9000: loss = 0.12347263097763062\n",
            "step = 9000: Average Return = 1.1100000143051147\n",
            "This is a new best: 1.1100000143051147\n",
            "step = 9250: loss = 0.1316603273153305\n",
            "step = 9500: loss = 0.11903399974107742\n",
            "step = 9500: Average Return = 1.100000023841858\n",
            "Returning to previous policy...\n",
            "step = 9750: loss = 0.11282666027545929\n",
            "step = 10000: loss = 0.2217157781124115\n",
            "step = 10000: Average Return = 1.0499999523162842\n",
            "Returning to previous policy...\n",
            "step = 10250: loss = 0.16511166095733643\n",
            "step = 10500: loss = 0.15507587790489197\n",
            "step = 10500: Average Return = 0.9900000095367432\n",
            "Returning to previous policy...\n",
            "step = 10750: loss = 0.14302976429462433\n",
            "step = 11000: loss = 0.15390826761722565\n",
            "step = 11000: Average Return = 1.1399999856948853\n",
            "This is a new best: 1.1399999856948853\n",
            "step = 11250: loss = 0.11176826804876328\n",
            "step = 11500: loss = 0.17444558441638947\n",
            "step = 11500: Average Return = 0.9399999976158142\n",
            "Returning to previous policy...\n",
            "step = 11750: loss = 0.15002334117889404\n",
            "step = 12000: loss = 0.16110819578170776\n",
            "step = 12000: Average Return = 0.9700000286102295\n",
            "Returning to previous policy...\n",
            "step = 12250: loss = 0.17941632866859436\n",
            "step = 12500: loss = 0.1635097712278366\n",
            "step = 12500: Average Return = 1.0299999713897705\n",
            "Returning to previous policy...\n",
            "step = 12750: loss = 0.1498984843492508\n",
            "step = 13000: loss = 0.1838221251964569\n",
            "step = 13000: Average Return = 0.9300000071525574\n",
            "Returning to previous policy...\n",
            "step = 13250: loss = 0.15797093510627747\n",
            "step = 13500: loss = 0.152765154838562\n",
            "step = 13500: Average Return = 1.0399999618530273\n",
            "Returning to previous policy...\n",
            "step = 13750: loss = 0.13455863296985626\n",
            "step = 14000: loss = 0.1262943595647812\n",
            "step = 14000: Average Return = 0.9700000286102295\n",
            "Returning to previous policy...\n",
            "step = 14250: loss = 0.18613982200622559\n",
            "step = 14500: loss = 0.12195192277431488\n",
            "step = 14500: Average Return = 0.9100000262260437\n",
            "Returning to previous policy...\n",
            "step = 14750: loss = 0.17625631392002106\n",
            "step = 15000: loss = 0.15451253950595856\n",
            "step = 15000: Average Return = 1.0299999713897705\n",
            "Returning to previous policy...\n",
            "step = 15250: loss = 0.17991454899311066\n",
            "step = 15500: loss = 0.11608613282442093\n",
            "step = 15500: Average Return = 1.0299999713897705\n",
            "Returning to previous policy...\n",
            "step = 15750: loss = 0.16077262163162231\n",
            "step = 16000: loss = 0.12193754315376282\n",
            "step = 16000: Average Return = 1.0499999523162842\n",
            "Returning to previous policy...\n",
            "step = 16250: loss = 0.13109295070171356\n",
            "step = 16500: loss = 0.18876945972442627\n",
            "step = 16500: Average Return = 1.100000023841858\n",
            "Returning to previous policy...\n",
            "step = 16750: loss = 0.2094549685716629\n",
            "step = 17000: loss = 0.17035704851150513\n",
            "step = 17000: Average Return = 0.9800000190734863\n",
            "Returning to previous policy...\n",
            "step = 17250: loss = 0.14625167846679688\n",
            "step = 17500: loss = 0.13792896270751953\n",
            "step = 17500: Average Return = 0.9800000190734863\n",
            "Returning to previous policy...\n",
            "step = 17750: loss = 0.1346570998430252\n",
            "step = 18000: loss = 0.12864930927753448\n",
            "step = 18000: Average Return = 0.8999999761581421\n",
            "Returning to previous policy...\n",
            "step = 18250: loss = 0.15766148269176483\n",
            "step = 18500: loss = 0.14594683051109314\n",
            "step = 18500: Average Return = 1.0\n",
            "Returning to previous policy...\n",
            "step = 18750: loss = 0.1604437232017517\n",
            "step = 19000: loss = 0.14485414326190948\n",
            "step = 19000: Average Return = 0.9700000286102295\n",
            "Returning to previous policy...\n",
            "step = 19250: loss = 0.12816846370697021\n",
            "step = 19500: loss = 0.13743308186531067\n",
            "step = 19500: Average Return = 1.1799999475479126\n",
            "This is a new best: 1.1799999475479126\n",
            "step = 19750: loss = 0.15696799755096436\n",
            "step = 20000: loss = 0.12597179412841797\n",
            "step = 20000: Average Return = 0.9800000190734863\n",
            "Returning to previous policy...\n",
            "step = 20250: loss = 0.1567949801683426\n",
            "step = 20500: loss = 0.1697821468114853\n",
            "step = 20500: Average Return = 1.0299999713897705\n",
            "Returning to previous policy...\n",
            "step = 20750: loss = 0.1726672649383545\n",
            "step = 21000: loss = 0.12997081875801086\n",
            "step = 21000: Average Return = 0.949999988079071\n",
            "Returning to previous policy...\n",
            "step = 21250: loss = 0.132853165268898\n",
            "step = 21500: loss = 0.1647592931985855\n",
            "step = 21500: Average Return = 0.8600000143051147\n",
            "Returning to previous policy...\n",
            "step = 21750: loss = 0.175176203250885\n",
            "step = 22000: loss = 0.13752934336662292\n",
            "step = 22000: Average Return = 0.8999999761581421\n",
            "Returning to previous policy...\n",
            "step = 22250: loss = 0.17644982039928436\n",
            "step = 22500: loss = 0.1834685504436493\n",
            "step = 22500: Average Return = 1.0199999809265137\n",
            "Returning to previous policy...\n",
            "step = 22750: loss = 0.13850617408752441\n",
            "step = 23000: loss = 0.10013028234243393\n",
            "step = 23000: Average Return = 1.0299999713897705\n",
            "Returning to previous policy...\n",
            "step = 23250: loss = 0.19774426519870758\n",
            "step = 23500: loss = 0.15078388154506683\n",
            "step = 23500: Average Return = 0.9300000071525574\n",
            "Returning to previous policy...\n",
            "step = 23750: loss = 0.13550174236297607\n",
            "step = 24000: loss = 0.17425410449504852\n",
            "step = 24000: Average Return = 1.059999942779541\n",
            "Returning to previous policy...\n",
            "step = 24250: loss = 0.13060683012008667\n",
            "step = 24500: loss = 0.14382509887218475\n",
            "step = 24500: Average Return = 0.8999999761581421\n",
            "Returning to previous policy...\n",
            "step = 24750: loss = 0.1446896195411682\n",
            "step = 25000: loss = 0.13632753491401672\n",
            "step = 25000: Average Return = 0.9300000071525574\n",
            "Returning to previous policy...\n",
            "step = 25250: loss = 0.15158531069755554\n",
            "step = 25500: loss = 0.1400512456893921\n",
            "step = 25500: Average Return = 1.0299999713897705\n",
            "Returning to previous policy...\n",
            "step = 25750: loss = 0.1566278487443924\n",
            "step = 26000: loss = 0.13610564172267914\n",
            "step = 26000: Average Return = 0.9900000095367432\n",
            "Returning to previous policy...\n",
            "step = 26250: loss = 0.15010587871074677\n",
            "step = 26500: loss = 0.18208564817905426\n",
            "step = 26500: Average Return = 1.0\n",
            "Returning to previous policy...\n",
            "step = 26750: loss = 0.16367724537849426\n",
            "step = 27000: loss = 0.11710953712463379\n",
            "step = 27000: Average Return = 0.9399999976158142\n",
            "Returning to previous policy...\n",
            "step = 27250: loss = 0.154246985912323\n",
            "step = 27500: loss = 0.14417141675949097\n",
            "step = 27500: Average Return = 0.9200000166893005\n",
            "Returning to previous policy...\n",
            "step = 27750: loss = 0.13491548597812653\n",
            "step = 28000: loss = 0.17196570336818695\n",
            "step = 28000: Average Return = 1.100000023841858\n",
            "Returning to previous policy...\n",
            "step = 28250: loss = 0.12540267407894135\n",
            "step = 28500: loss = 0.12452563643455505\n",
            "step = 28500: Average Return = 1.059999942779541\n",
            "Returning to previous policy...\n",
            "step = 28750: loss = 0.18795478343963623\n",
            "step = 29000: loss = 0.1307162195444107\n",
            "step = 29000: Average Return = 1.059999942779541\n",
            "Returning to previous policy...\n",
            "step = 29250: loss = 0.15755973756313324\n",
            "step = 29500: loss = 0.1377999186515808\n",
            "step = 29500: Average Return = 1.100000023841858\n",
            "Returning to previous policy...\n",
            "step = 29750: loss = 0.16171127557754517\n",
            "step = 30000: loss = 0.1392158567905426\n",
            "step = 30000: Average Return = 1.1299999952316284\n",
            "Returning to previous policy...\n",
            "step = 30250: loss = 0.15615986287593842\n",
            "step = 30500: loss = 0.1620255559682846\n",
            "step = 30500: Average Return = 0.9900000095367432\n",
            "Returning to previous policy...\n",
            "step = 30750: loss = 0.15614724159240723\n",
            "step = 31000: loss = 0.16748444736003876\n",
            "step = 31000: Average Return = 1.0099999904632568\n",
            "Returning to previous policy...\n",
            "step = 31250: loss = 0.11919326335191727\n",
            "step = 31500: loss = 0.15044718980789185\n",
            "step = 31500: Average Return = 1.1299999952316284\n",
            "Returning to previous policy...\n",
            "step = 31750: loss = 0.15216246247291565\n",
            "step = 32000: loss = 0.11877381801605225\n",
            "step = 32000: Average Return = 0.9300000071525574\n",
            "Returning to previous policy...\n",
            "step = 32250: loss = 0.1670638471841812\n",
            "step = 32500: loss = 0.1421576291322708\n",
            "step = 32500: Average Return = 0.949999988079071\n",
            "Returning to previous policy...\n",
            "step = 32750: loss = 0.16625264286994934\n",
            "step = 33000: loss = 0.1315876990556717\n",
            "step = 33000: Average Return = 0.9700000286102295\n",
            "Returning to previous policy...\n",
            "step = 33250: loss = 0.15354202687740326\n",
            "step = 33500: loss = 0.15767158567905426\n",
            "step = 33500: Average Return = 1.0199999809265137\n",
            "Returning to previous policy...\n",
            "step = 33750: loss = 0.1635127067565918\n",
            "step = 34000: loss = 0.13423459231853485\n",
            "step = 34000: Average Return = 1.0399999618530273\n",
            "Returning to previous policy...\n",
            "step = 34250: loss = 0.18673379719257355\n",
            "step = 34500: loss = 0.15545965731143951\n",
            "step = 34500: Average Return = 0.9700000286102295\n",
            "Returning to previous policy...\n",
            "step = 34750: loss = 0.17425395548343658\n",
            "step = 35000: loss = 0.16788029670715332\n",
            "step = 35000: Average Return = 0.9599999785423279\n",
            "Returning to previous policy...\n",
            "step = 35250: loss = 0.1391971856355667\n",
            "step = 35500: loss = 0.15194609761238098\n",
            "step = 35500: Average Return = 1.0\n",
            "Returning to previous policy...\n",
            "step = 35750: loss = 0.1377924382686615\n",
            "step = 36000: loss = 0.15682919323444366\n",
            "step = 36000: Average Return = 0.9200000166893005\n",
            "Returning to previous policy...\n",
            "step = 36250: loss = 0.11202598363161087\n",
            "step = 36500: loss = 0.11393211036920547\n",
            "step = 36500: Average Return = 0.9599999785423279\n",
            "Returning to previous policy...\n",
            "step = 36750: loss = 0.1494428515434265\n",
            "step = 37000: loss = 0.12006260454654694\n",
            "step = 37000: Average Return = 1.100000023841858\n",
            "Returning to previous policy...\n",
            "step = 37250: loss = 0.14243309199810028\n",
            "step = 37500: loss = 0.13475839793682098\n",
            "step = 37500: Average Return = 1.090000033378601\n",
            "Returning to previous policy...\n",
            "step = 37750: loss = 0.1327623426914215\n",
            "step = 38000: loss = 0.19083836674690247\n",
            "step = 38000: Average Return = 1.0800000429153442\n",
            "Returning to previous policy...\n",
            "step = 38250: loss = 0.13225503265857697\n",
            "step = 38500: loss = 0.11816282570362091\n",
            "step = 38500: Average Return = 1.100000023841858\n",
            "Returning to previous policy...\n",
            "step = 38750: loss = 0.14715687930583954\n",
            "step = 39000: loss = 0.15042023360729218\n",
            "step = 39000: Average Return = 1.0399999618530273\n",
            "Returning to previous policy...\n",
            "step = 39250: loss = 0.14648494124412537\n",
            "step = 39500: loss = 0.15923212468624115\n",
            "step = 39500: Average Return = 0.9100000262260437\n",
            "Returning to previous policy...\n",
            "step = 39750: loss = 0.1587541550397873\n",
            "step = 40000: loss = 0.13198505342006683\n",
            "step = 40000: Average Return = 0.9700000286102295\n",
            "Returning to previous policy...\n",
            "step = 40250: loss = 0.13757644593715668\n",
            "step = 40500: loss = 0.1523238718509674\n",
            "step = 40500: Average Return = 0.9599999785423279\n",
            "Returning to previous policy...\n",
            "step = 40750: loss = 0.15172331035137177\n",
            "step = 41000: loss = 0.11898728460073471\n",
            "step = 41000: Average Return = 1.0199999809265137\n",
            "Returning to previous policy...\n",
            "step = 41250: loss = 0.19457800686359406\n",
            "step = 41500: loss = 0.16107164323329926\n",
            "step = 41500: Average Return = 1.0499999523162842\n",
            "Returning to previous policy...\n",
            "step = 41750: loss = 0.1573992371559143\n",
            "step = 42000: loss = 0.13813014328479767\n",
            "step = 42000: Average Return = 0.9800000190734863\n",
            "Returning to previous policy...\n",
            "step = 42250: loss = 0.15794111788272858\n",
            "step = 42500: loss = 0.12541119754314423\n",
            "step = 42500: Average Return = 0.9399999976158142\n",
            "Returning to previous policy...\n",
            "step = 42750: loss = 0.1760859191417694\n",
            "step = 43000: loss = 0.1521841436624527\n",
            "step = 43000: Average Return = 1.1200000047683716\n",
            "Returning to previous policy...\n",
            "step = 43250: loss = 0.14756153523921967\n",
            "step = 43500: loss = 0.14128729701042175\n",
            "step = 43500: Average Return = 1.0\n",
            "Returning to previous policy...\n",
            "step = 43750: loss = 0.1526830643415451\n",
            "step = 44000: loss = 0.14055973291397095\n",
            "step = 44000: Average Return = 0.9399999976158142\n",
            "Returning to previous policy...\n",
            "step = 44250: loss = 0.13284508883953094\n",
            "step = 44500: loss = 0.13841557502746582\n",
            "step = 44500: Average Return = 0.949999988079071\n",
            "Returning to previous policy...\n",
            "step = 44750: loss = 0.12842188775539398\n",
            "step = 45000: loss = 0.12696096301078796\n",
            "step = 45000: Average Return = 0.9900000095367432\n",
            "Returning to previous policy...\n",
            "step = 45250: loss = 0.14381635189056396\n",
            "step = 45500: loss = 0.16383741796016693\n",
            "step = 45500: Average Return = 1.0499999523162842\n",
            "Returning to previous policy...\n",
            "step = 45750: loss = 0.15445001423358917\n",
            "step = 46000: loss = 0.16025327146053314\n",
            "step = 46000: Average Return = 0.9599999785423279\n",
            "Returning to previous policy...\n",
            "step = 46250: loss = 0.1414504051208496\n",
            "step = 46500: loss = 0.14918331801891327\n",
            "step = 46500: Average Return = 1.0199999809265137\n",
            "Returning to previous policy...\n",
            "step = 46750: loss = 0.14106056094169617\n",
            "step = 47000: loss = 0.15340358018875122\n",
            "step = 47000: Average Return = 1.1100000143051147\n",
            "Returning to previous policy...\n",
            "step = 47250: loss = 0.1567516326904297\n",
            "step = 47500: loss = 0.1544777899980545\n",
            "step = 47500: Average Return = 1.0299999713897705\n",
            "Returning to previous policy...\n",
            "step = 47750: loss = 0.1567888706922531\n",
            "step = 48000: loss = 0.1867634356021881\n",
            "step = 48000: Average Return = 0.9900000095367432\n",
            "Returning to previous policy...\n",
            "step = 48250: loss = 0.11933514475822449\n",
            "step = 48500: loss = 0.14425639808177948\n",
            "step = 48500: Average Return = 0.9300000071525574\n",
            "Returning to previous policy...\n",
            "step = 48750: loss = 0.15626227855682373\n",
            "step = 49000: loss = 0.12259866297245026\n",
            "step = 49000: Average Return = 0.9200000166893005\n",
            "Returning to previous policy...\n",
            "step = 49250: loss = 0.15159235894680023\n",
            "step = 49500: loss = 0.1425582319498062\n",
            "step = 49500: Average Return = 0.9800000190734863\n",
            "Returning to previous policy...\n",
            "step = 49750: loss = 0.11190212517976761\n",
            "step = 50000: loss = 0.13193286955356598\n",
            "step = 50000: Average Return = 0.9900000095367432\n",
            "Returning to previous policy...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5n65SOcMtIh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0797124b-8ec7-42b4-f736-9c3b4f81b225"
      },
      "source": [
        "compute_avg_return(eval_env, agent.policy, 2,True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NEW EPISODE:\n",
            "------------\n",
            "TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0]], dtype=int32)>,\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})\n",
            "This is the move I played: 6\n",
            "[0 0 1 0 0 0 1]\n",
            "This is the move I played: 3\n",
            "[0 1 1 0 1 0 1]\n",
            "This is the move I played: 3\n",
            "[1 1 1 1 1 1 1]\n",
            "REWARD FOR THAT EPISODE: 0.0\n",
            "NEW EPISODE:\n",
            "------------\n",
            "TimeStep(\n",
            "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0]], dtype=int32)>,\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})\n",
            "This is the move I played: 6\n",
            "[0 0 1 0 0 0 1]\n",
            "This is the move I played: 3\n",
            "[0 0 1 1 1 0 1]\n",
            "This is the move I played: 1\n",
            "[0 1 1 1 1 0 1]\n",
            "This is the move I played: 3\n",
            "[1 1 1 1 1 1 1]\n",
            "REWARD FOR THAT EPISODE: 1.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2FKq4cmmZPW",
        "outputId": "70ed08e1-c93e-4fa7-9633-9b266badf3c9"
      },
      "source": [
        "compute_avg_return(eval_env, agent.policy, 1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.003"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68jNcA_TiJDq"
      },
      "source": [
        "## Visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO-LWCdbbOIC"
      },
      "source": [
        "### Plots\n",
        "\n",
        "Use `matplotlib.pyplot` to chart how the policy improved during training.\n",
        "\n",
        "One iteration of `Cartpole-v0` consists of 200 time steps. The environment gives a reward of `+1` for each step the pole stays up, so the maximum return for one episode is 200. The charts shows the return increasing towards that maximum each time it is evaluated during training. (It may be a little unstable and not increase monotonically each time.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxtL1mbOYCVO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "075f5af6-ffaa-4e0f-b91e-71e58ef38285"
      },
      "source": [
        "iterations = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(iterations, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylim(top=15)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.36100000888109207, 15.0)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}